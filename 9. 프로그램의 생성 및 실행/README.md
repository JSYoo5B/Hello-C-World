# 프로그램의 생성 및 실행

이 전 챕터들을 학습하며 간단한 C 프로그램을 작성하는 것에는 어느정도 익숙해졌을 것이다. 그런데 변수나 함수 등 챕터에서 설명들을 보면 메모리가 어쩌느니 영역이 어쩌느니 하는 설명들이 등장하며 혼란을 초래했을 것이다. 내용을 어렵게 만들어 독자분들을 골탕먹이려는 속셈이 아니냐 생각할 수 있겠지만, 동작 원리와 같은 것들을 명확하게 설명하기 위해서는 필수불가결한 내용들이었다.  
여태까지는 중간에 컴퓨터나 프로그램의 동작 원리와 같은 내용을 추가하여 설명 돌려막기를 시전하였지만 이후 내용부터는 이러한 설명 방식에 한계가 있을 것으로 보인다. C언어는 운영체제를 만들기 위해 탄생한 언어인 만큼 컴퓨터를 제어하기 위한 막강한 기능들이 포함되어 있다. 이러한 기능들을 명확히 이해하기 위해서는 컴퓨터 아키텍처와 프로그램 동작 방식에 대한 이해가 필요하다.  
따라서 해당 챕터에서는 추후 내용을 원활하게 이해할 수 있도록 컴퓨터 및 프로그램에 대한 설명을 진행할 것이다. 물론 이러한 내용들을 상세하게 설명하면 그 자체로 여러 권의 책을 집필할 수 있을 분량이고 학습에도 부담이 있을 것이므로 되도록 최소 한도의 내용만 진행할 것이다.  
이 외에도 추후 학습에 도움이 될 컴파일러와 디버거 사용법들도 어느 정도 소개할 것인데, 너무 부담 가질 필요는 없다. 일단은 이러한 내용도 있다고 적당히 알아두고 필요할 때마다 해당 챕터를 돌아보거나 검색하면서 진행하여도 충분하다.  

## 컴퓨터 구조 찍어먹기

기본적으로 컴퓨터는 CPU를 중심으로 여러 장치들이 모여서 구성된다. 목적과 환경에 따라 구성이 크게 달라지기에 우선 일반적인 데스크탑 환경을 가정하면 다음과 같은 요소들이 포함된다.  

* CPU(Central Processing Unit) : 중앙 처리 장치는 명령어를 수행하여 컴퓨터 시스템을 제어하는 가장 핵심적인 장치이다. 일반적으로 ALU, 제어 장치, 레지스터와 같은 요소로 구성된다. 이러한 기본적인 요소들을 모듈화하여 만든 칩을 마이크로프로세서(microprocessor)라고도 하며, 마이크로프로세서에 입출력 모듈과 같은 외부 제어 요소들을 통합하여 만든 칩을 마이크로컨트롤러(microcontroller)라고도 한다.  
    * ALU(Arithmetic and Logic Unit) : 산술 연산, 논리 연산 등을 담당하는 연산 장치이다.
    * 제어 장치(Control Unit) : 주로 명령어를 읽고 해석하며 데이터 처리를 위한 기능들을 제어하기 위한 장치이다.
    * 레지스터(Register) : 프로세서 내에서 자료를 보관하고 처리하기 위한 아주 빠른 기억 장소이다. 크게 범용 레지스터와 특수 레지스터로 구분한다.
        * 범용 레지스터 : 범용적인 데이터 처리를 위하여 데이터 및 주소를 저장한다.
        * 특수 레지스터 : 특수한 기능을 위해 용도가 정해져 특정한 데이터 및 주소를 저장한다.
* 캐시 메모리 : 데이터를 미리 복사해둔 임시 저장소로 데이터 요청이 발생했을 때 빠르게 응답하기 위하여 사용한다. 캐시는 다양한 하드웨어 및 소프트웨어의 구성 요소로 사용된다. 예를 들어 메인 메모리에 있는 데이터를 CPU에게 빠르게 전달하기 위한 캐시를 CPU 캐시라고 하며, 디스크에 있는 데이터를 빠르게 메모리 등 외부로 전달하거나 디스크 쓰기 요청을 빠르게 수행하기 위한 캐시를 디스크 캐시라고 한다.
* RAM(Random Access Memory) 메모리 : 임의의 영역에 접근하여 읽고 쓰기가 가능한 주기억장치(메인메모리)로, 일반적으로 전력이 공급되지 않으면 데이터가 사라지는 휘발성 메모리이다. 어느 위치에 접근하든지 동일한 시간이 걸리기 때문에 랜덤 액세스 메모리라고 부른다.
* HDD/SSD : 전력이 공급되지 않아도 데이터가 휘발되지 않는 비휘발성 메모리로 보조 기억 장치로 사용한다.
    * HDD(Hard Disk Drive) : 플래터를 회전하여 자기 패턴으로 정보를 기록하는 메모리로, 데이터가 저장된 위치에 따라 접근 시간이 달라진다.
    * SSD(Solid State Drive) : 일반적으로 플래시 메모리를 사용하여 전자식으로 정보를 기록하는 메모리로, 데이터가 저장된 위치에 따른 접근 시간이 HDD에 비해 상대적으로 균일하다.
* GPU(Graphics Processing Unit) : 그래픽 연산을 빠르게 처리하여 결과 값을 모니터에 출력하기 위한 연산 장치이다. CPU와 비교하여 굉장히 많은 수의 연산 장치가 간단한 구조로 내장되어 있기 때문에 처리할 데이터가 많은 연산을 수행하기에 적합하다.
* 메인보드(Mainboard/Motherboard) : 컴퓨터 시스템의 구성 요소들을 장착할 수 있는 슬롯을 제공하며 해당 요소들에 전원을 공급하고 요소들간에 신호를 주고받는 통로인 버스를 제공하는 전자기판이다.
    * 버스(bus) : 컴퓨터 시스템의 구성 요소들 간에 데이터와 정보를 전송하기 위한 통로이다.

일반적으로 프로그램은 외부 저장 장치에 저장되어 있다가 실행되면 주기억장치인 RAM에 로드된다. 주기억장치로 올라온 프로그램의 명령어와 데이터들은 버스를 통해 CPU로 이동하여 명령을 수행한다. 이렇게 범용적인 하드웨어 구조에 프로그램을 내장시키고 메모리로부터 명령어와 데이터를 CPU로 가져오는 구조를 폰 노이만 아키텍처(Von Neumann Architecture)라고 한다.  

그런데 컴퓨터가 발전할수록 CPU의 연산 속도는 굉장히 빠르게 증가했지만 메모리 접근 속도는 상대적으로 느리게 증가했다. 이러한 불균형으로 생기는 병목 현상은 폰 노이만 아키텍처의 고질적인 문제인데, 이를 해소하기 위하여 CPU 내에 캐시 메모리를 두었으며 현재 많은 경우 세 단계의 캐시 메모리를 사용하고 있다.  

> 실제 캐시 메모리가 적용된 컴퓨터 시스템은 1950년대에서 1960년대에 등장하였는데, 폰 노이만은 1946년 그의 논문에서 이미 캐시 메모리의 필요성에 대해 예견하였다.  

그리고 CPU의 연산 속도 또한 증가하다보니 과도한 에너지 소모와 발열량으로 인하여 어느 정도 한계를 맞이하였다. 이러한 한계를 극복하기 위하여 다수의 연산 처리 장치 모듈을 하나의 칩에 내장한 멀티 코어 CPU가 등장하였다. 연산 처리 장치 모듈을 코어라고 하며, 각 코어는 독자적이고 병렬적으로 명령어를 수행할 수 있다.  

다시 캐시 이야기로 돌아와서 세 단계의 CPU 캐시 메모리가 구성된다고 했을 때, CPU와 가장 가깝고 가장 빠르게 접근할 수 있는 캐시를 L1 캐시라고 하며, CPU로부터 가장 멀리 있는 캐시를 L3 캐시라고 한다. 이 경우 CPU는 메모리에 접근할 때 RAM에 직접 접근하지 않고 항상 캐시 메모리로 접근한다. CPU가 원하는 데이터 혹은 명령어가 L1 캐시에 있으면 바로 불러오고, 없다면 L2 캐시로부터 L1 캐시로 불러온 후 L1 캐시로부터 불러온다. L2 캐시에도 데이터가 없다면 L3 캐시로부터 L2 캐시로 불러오고, L3 캐시에도 데이터가 없다면 RAM에서 L3 캐시로 불러오는 방식이다. 일반적으로 L1 캐시와 L2 캐시는 CPU 코어마다 존재하고, L3 캐시는 CPU 코어들이 공유하여 사용한다.  

위에 CPU의 처리 속도와 메모리 접근 속도의 차이로 인한 병목 현상을 이야기 했는데, 폰 노이만 아키텍처의 유사한 고질적인 문제로 데이터와 명령어에 동시에 접근할 수 없다는 점이 있다. 고전적인 폰 노이만 아키텍처에서는 데이터 메모리와 명령어 메모리가 구분되어 있지 않고 같은 버스를 통해 CPU로 이동하기 때문에 데이터와 명령어를 병렬적으로 가져올 수 없다. 이러한 구조는 다음 명령어를 가져오거나 실행하려는 명령어에서 필요한 데이터를 가져오는데에 약점을 갖는다.  
이를 극복하기 위하여 명령어를 위한 버스와 데이터를 위한 버스를 물리적으로 분리한 구조를 하버드 아키텍처(Harvard Architecture)라고 한다. 하버드 아키텍처에서는 실행하려는 명령어를 끊기지 않고 가져올 수 있으므로 더 빠르게 명령어를 처리할 수 있다. 하지만 완전한 하버드 아키텍처를 구현하기 위해서는 많은 회로와 메모리 분리가 필요하며 이로 인해 비용과 복잡성이 늘어난다. 그 외에도 하드웨어의 명령어 처리 과정에 있어 일부 제한들이 생길 수 있다.  

현대의 많은 CPU는 폰 노이만 아키텍처와 하버드 아키텍처를 절충하여 복잡성과 속도 문제에 적절히 대처하고 있다. L1 캐시는 데이터 캐시와 명령어 캐시로 나누고 코어 모듈에 전용 버스로 연결한다. 이에 반해 L3 캐시와 주기억장치는 데이터와 명령어를 위한 영역을 따로 나누지 않고 같은 버스를 통해 통신한다. 이 외에도 컴퓨터 시스템의 전반적인 성능을 증가시킬 방법이 있다면 적용하며 구조를 개선하고 있다.  

### 메모리 계층 구조

메모리 계층 구조(Memory Hierarchy)는 메모리의 종류를 속도, 용량, 성질 등에 의해 구분하여 필요에 따라 나누어 두는 것을 뜻한다. 일반적으로 다음과 같은 그림으로 나타내는데, 표현하고자 하는 바에 따라 구분 방식이 달라서 내용은 조금씩 다를 수도 있다.  
![memory_hierarchy](https://raw.githubusercontent.com/pr0gr4m/Hello-C-World/main/img/%EC%BB%B4%EA%B5%AC/memory.png)  

메모리는 하드웨어적인 구현 방법, 크기, CPU와의 물리적인 거리에 따라 접근 시간이 달라진다. 예를 들어 RAM도 기억 장치를 어떻게 구성하는지에 따라 동작 방식이 달라져서 SRAM(Static Random Access Memory)와 DRAM(Dynamic Random Access Memory)로 나뉜다. SRAM은 접근 속도가 빠르지만 대용량으로 만들기 어렵고 많은 비용이 든다. DRAM은 상대적으로 접근 속도가 느리지만 비교적으로 대용량으로 만들기 쉽고 적은 비용이 든다. 따라서 일반적으로 CPU 캐시 메모리는 SRAM으로, 주기억장치는 DRAM으로 제작한다. (집필 중인 현재 많은 CPU 캐시는 SRAM으로, 주기억장치를 보통 DRAM의 일종인 SDRAM으로 제작하고 있다. 물론 모두 그런 것은 아니다.) 그리고 같은 기억 소자를 사용한다고 하더라도 크기가 작을수록 접근 속도를 빠르게 만들기 쉽다. 예를 들어 동네 놀이터 작은 모래 사장에 떨어진 500원을 찾는 것과 드넓은 사막에 떨어진 500원을 찾는다고 할 때 어느 쪽이 빠르게 찾기 쉬운지 생각하면 이해하기 쉽다. 따라서 일반적으로 속도에 최적화된 설계를 한 SRAM을 L1 캐시로 사용하고, 크기에 최적화된 설계를 한 SRAM을 L2와 L3 캐시에 사용하고는 한다.  

정리하면 CPU와 가까이 있을 수록 빠르지만 용량이 적고, CPU에 멀리 있을 수록 느리지만 용량이 큰 기억 장치를 사용하고 있다. 이러한 상황에서 메모리를 효율적으로 사용하려면 어떻게 해야할까? 당연하지만 CPU가 미래에 자주 사용할 데이터일수록 빠른 메모리에 위치해두는 것이 좋다.  
그러면 어떤 데이터가 미래에 자주 사용할지 어떻게 알 수 있을까? 완벽한 예측을 할 수는 없지만 힌트가 될 만한 두 가지 규칙이 있다. '한 번 접근한 데이터는 미래에 다시 접근할 확률이 높다' 와 '방금 접근한 데이터 주변에 있는 데이터에 접근할 확률이 높다' 이다. 예를 들어 ```for (int i = 0; i < 100; i++) { sum += i; }``` 와 같은 문장이 있으면 변수 i에 자주 접근하는 것을 생각해보면 된다. 이러한 성질을 지역성(Locality) 라고 하는데 특히 전자 규칙을 시간적 지역성(Temporal Locality)라고 하며 후자 규칙을 공간적 지역성(Spatial Locality)라고 한다.  
지역성에 의거하여 어떠한 데이터를 필요할 때 인근 주소의 데이터들과 함께 상위 캐시 메모리로 복사되도록 만들면 속도와 공간 효율을 적당히 잡을 수 있다. 이런 식으로 데이터들을 캐시 메모리에 동적으로 복사해두고, CPU가 해당 데이터에 접근할 때 캐시 메모리에 존재하면 캐시 히트(Cache Hit)라고 하고 존재하지 않으면 캐시 미스(Cache Miss)라고 한다. 캐시 미스가 발생하면 하위 메모리에서 데이터를 찾아서 인근 데이터들과 함께 상위 메모리로 복사하는 작업을 반복하며 CPU가 메모리에 있는 데이터에 접근한다.  

현대의 컴퓨터는 대부분 이러한 메모리 계층 구조로 구성되어 있기 때문에, 프로그래밍 시 인접한 데이터 위주로 접근하여 캐시 히트율을 높여주면 프로그램의 성능을 향상시킬 수 있다. 이렇게 최적화된 프로그램 코드를 캐시 친화적인 코드(Cache Friendly Code)라고 한다.  

### 명령어 집합 구조

명령어 집합 구조(Instruction Set Architecture)는 좁게는 CPU가 실행할 수 있는 명령어들의 집합 구조를 의미하며, 넓게는 컴퓨터 시스템에 대한 추상적인 구조를 의미한다. 특정한 명령어 집합 구조를 실행할 수 있도록 구현한 장치가 CPU인 것이다. 즉, 명령어 집합 구조가 곧 컴퓨터 구조(Computer Architecture)인 것이다.  

컴퓨터 구조와 메모리 이야기를 하다가 뜬금 없이 명령어 이야기를, 그것도 굉장히 이론적으로 설명하여 무슨 소리인가 싶을 수 있다. 명령어 집합 구조(이 후 ISA로 표기)는 컴퓨터 구조에 있어서 가장 근본적인 부분이기에 컴퓨터 구조에 대한 이야기를 할 때 ISA가 무엇인지 알고 있다는 가정을 하는 경우가 대부분이다. 따라서 다음 내용들을 진행하기 전에 간단하게라도 ISA에 대한 이야기를 하려고 한다. 물론 최대한 간단한 수준에서 이야기할 것이다.  

여기까지 읽은 독자들은 이제 알고 있겠지만, 모든 명령어는 결국 이진수일 뿐이며 이를 특정한 규칙에 의해 CPU가 해석하여 실행할 뿐이다. 그러면 그 규칙을 어떻게 세울 수 있을까?  

예를 들어 컴퓨터를 통해서 더하기, 빼기, 곱하기, 나누기 네 가지의 연산을 수행하려고 한다고 가정해보자. 지원해야 하는 명령은 네 가지 종류가 되는 것이다. 그리고 모든 명령어는 32비트의 크기를 갖는다고 하자.  
우선 명령어의 처음 두 비트는 명령어의 종류를 구분하는데 사용한다고 정하자. 처음 두 비트가 00이라면 더하기, 01이라면 빼기, 10이라면 곱하기, 11이라면 나누기다.  

위의 사칙 연산들은 모두 두 개의 피연산자를 필요로 한다. 그리고 연산을 수행했다면 그 결과를 저장할 장소도 필요할 것이다. 32비트 중 두 비트는 명령어의 종류를 구분하는데 사용하기로 하였으니, 30비트가 남았다. 30 비트를 셋으로 나눠서 앞의 10 비트는 결과, 나머지 두 개의 10비트는 피연산자로 사용하기로 한다.  

그러면 3 + 2라는 결과를 어딘가에 저장하는 명령어는 ```00 ?????????? 0000000011 0000000010``` 이 될 것이다. 물음표로 되어있는 열 개의 비트를 어떻게 채우면 될까?  

이 질문에 답하려면 결과를 어디에 저장할지를 생각해봐야 한다. 우선 레지스터와 주기억장치 두 가지 후보지를 생각해볼 수 있다. 둘 중 하나를 꼭 선택해야할까? 그럴 필요 없이 열 개의 비트를 둘로 나눠보자. 열 개의 비트 중 앞의 두 개의 비트가 10일때는 레지스터를, 11일때는 주소를 의미하도록 한다.  
그러면 ```1000000000```은 0번 레지스터, ```1000000001```은 1번 레지스터, ```1100000000```은 주기억장치 주소 0번지, ```1100000010```는 주기억장치 주소 2번지가 되는 것이다.  
이제 명령어 ```00 1000000001 0000000011 0000000010``` 라고 하면 3과 2를 더해서 1번 레지스터에 저장하라는 의미이다.  

그런데 피연산자에 상수만 사용할 수 있는 것이 조금 아쉽다. 피연산자 비트도 둘로 나눠서 앞의 두 개의 비트가 00일때는 상수를, 10일때는 레지스터를, 11일때는 주소를 의미하도록 해보자. 그러면 ```00 1000000001 1000000000 0000000100```라는 명령어는 0번 레지스터의 값에 4를 더해서 1번 레지스터에 저장하라는 의미가 된다.  
같은 방식으로 ```01 1100011000 1000000011 0000001100``` 라고 한다면 3번 레지스터의 값에서 12를 뺀 값을 주기억장치 주소 24번지에 저장하라 라고 해석할 수 있다.  

정말 간단하고 어설프지만 명령어 집합을 정의해보았다. 실제 명령어들은 당연히 이와 비교도 안되게 복잡하다. 그런데 이 명령어 구조가 얼마나 복잡한지에 따라 ISA가 크게 구분된다.  

복잡한 명령어 체계를 가져서 하나의 명령어가 여러 기능을 수행하는 명령어 집합을 CISC(Complex Instruction Set Computer)라고 한다. 데스크탑 컴퓨터로 흔히 사용하는 인텔 CPU의 x86 아키텍처나 AMD CPU의 amd64 아키텍처가 대표적인 CISC 아키텍처이다.  
CISC 명령어는 보통 명령어의 길이가 가변적이며 복잡하다. 예를 들어 더하기 명령어만 하더라도 여러 형태가 있고 각 형태마다 명령어의 길이가 다르다. 따라서 명령어를 해석하는 데 시간이 오래 걸리고 명령어를 해석하기 위한 회로도 복잡하다. 하지만 명령어가 복잡한만큼 다양한 명령어를 지원하기 때문에 기능이 막강하며 고수준 프로그래밍 언어에 대한 표현력이 좋다. 쉽게 말해 C언어 문장 하나를 적은 개수의 명령어로 표현할 수 있다. 참고로 C언어로 만든 최초의 소프트웨어인 유닉스4를 기동한 PDP-11 머신도 CISC 아키텍처 컴퓨터이다.  

이와 상반되어 비교적 간단하고 적은 수의 명령어 체계를 가지고 개별 명령어를 빠르게 실행할 수 있도록 만든 명령어 집합을 RISC(Reduced Instruction Set Computer)라고 한다. 스마트폰이나 임베디드 기기에서 많이 사용하는 ARM CPU의 aarch64/arm64 아키텍처가 대표적인 RISC 아키텍처이다. 최근 애플에서 선풍적인 인기를 끌고 있는 m 시리즈와 같은 실리콘 제품들도 aarch64 아키텍처를 사용하고 있다. 이 외에도 대표적인 RISC 아키텍처로 특정한 라이센스 하에서 자유롭게 사용하며 누구나 자발적으로 기여에 참여할 수 있는 RISC-V 아키텍처가 있다.  
일반적으로 RISC 아키텍처에서 명령어는 고정적인 길이를 가지고 빠르게 수행할 수 있도록 설계된다. 명령어의 종류 자체도 굉장히 적으며, 특히 메모리 접근을 위한 명령어는 로드(load)와 스토어(store)로 제한된다. 따라서 CPU는 각각의 RISC 명령을 쉽고 빠르게 실행할 수 있다. 반대로 명령어가 단순한만큼 고수준 프로그래밍 언어에 대한 표현력은 좋지 않아서 컴파일 과정이 어렵다. 이번에도 다시 말해 C언어 문장 하나를 많은 개수의 명령어로 표현해야 하는 것이다.  

이렇게만 보면 두 명령어 체계가 완전히 반대 성격만 가지고 있는 것으로 보인다. 근본적으로 CISC는 고수준 프로그래밍 언어를 기계어로 번역하는 컴파일에 유리하지만 CPU에서 실행하는데 불리하고, 반대로 RISC는 컴파일에는 불리하지만 CPU에서의 실행에는 유리하기에 맞는 이야기이다.  
하지만 컴퓨터 연구가들은 절대로 단점을 그대로 내버려두려고 하지 않는다. 현대 CISC를 구현한 CPU의 경우 대부분 내부적으로 CISC 명령을 실행하기 전에 micro-operation이라는 단순한 명령어로 분리하여 RISC 명령어처럼 실행한다. RISC의 경우 하드웨어의 발전으로 조금씩 복잡한 명령어도 지원하며, 컴파일 기술도 발전하면서 자연스럽게 단점을 상쇄시켜갔다. 이로써 두 명령어 체계가 서로 완전히 다른 성격을 갖고 있지만, 내부 구현은 상호 이점을 흡수할 수 있도록 점점 유사한 형태로 발전하고 있다.  

그렇다면 이러한 구분이 별로 중요하지 않은 것일까? 사실 현대에 와서는 CISC와 RISC의 구분은 의미가 조금씩 퇴색되었다. 그보다 중요한 것은 ISA 자체의 구분이다. 무슨 말이냐 하면 특정한 ISA를 타겟으로 만들어진 프로그램을 다른 ISA 컴퓨터에서 실행할 수 없다. 즉, 데스크탑 amd64 환경을 위해 만들어진 프로그램은 별도의 에뮬레이션같은 처리를 하지 않는 이상 스마트폰 aarch64 CPU에서 동작할 수 없으며, 반대로 스마트폰 aarch64 환경을 위해 만들어진 애플리케이션을 데스크탑 amd64 CPU에서 실행할 수 없다는 것이다.  

이러한 성질 때문에 C언어의 경우 각 아키텍처 환경마다 별도의 컴파일러를 제공한다. gcc의 경우 다음과 같이 명령어를 입력하면 각 환경을 위한 여러 종류의 컴파일러를 볼 수 있다.  
```bash
pr0gr4m@DESKTOP-IRB9MN5:~/src$ sudo apt search 'gcc-' --names-only | grep '^gcc-[^0-9]'

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

gcc-aarch64-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
gcc-alpha-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
gcc-arm-linux-gnueabi/focal 4:9.3.0-1ubuntu2 amd64
gcc-arm-linux-gnueabihf/focal 4:9.3.0-1ubuntu2 amd64
gcc-arm-none-eabi/focal 15:9-2019-q4-0ubuntu1 amd64
gcc-arm-none-eabi-source/focal 15:9-2019-q4-0ubuntu1 all
gcc-avr/focal 1:5.4.0+Atmel3.6.1-2build1 amd64
gcc-doc/focal 4:9.3.0-1ubuntu2 amd64
gcc-h8300-hms/focal 1:3.4.6+dfsg2-4.1 amd64
gcc-hppa-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
gcc-hppa64-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
gcc-i686-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
gcc-m68hc1x/focal 1:3.3.6+3.1+dfsg-3.1ubuntu1 amd64
gcc-m68k-linux-gnu/focal 4:9.3.0-1ubuntu2 amd64
... (생략)
```  

결과 중에 예시로 ```gcc-aarch64-linux-gnu```가 aarch64 아키텍처를 위한 컴파일러이다. 만약 인텔/AMD CPU를 사용하는 amd64 환경에서 프로그래밍하여 aarch64 아키텍처에서 동작하는 프로그램을 만들고 싶다면, 작성한 C언어 소스 코드를 평소에 사용하던 컴파일러가 아닌 이런 aarch64를 위한 컴파일러를 사용해서 컴파일해야 한다. 이렇게 컴파일러가 실행되는 환경이 아닌 다른 아키텍처/플랫폼 환경에서 실행하기 위한 코드를 생성하는 컴파일을 크로스 컴파일(cross compile)이라고 한다.  

## 프로그램의 생성 과정

어떤 언어와 개발 도구를 사용하는지에 따라 프로그램의 생성 과정은 조금씩 달라진다. 여기서는 서적의 성격에 맞게 C언어와 gcc 컴파일러를 사용하는 경우에 대해 이야기한다.  
여태까지 make 빌드 도구를 사용하여 프로그램을 생성했는데, 리눅스에서는 이러면 내부적으로 gcc를 기본 C 컴파일러로 사용한다. gcc를 사용하여 소스 파일을 컴파일하면 내부적으로 다음 그림과 같은 과정을 거친다.  
![compile](https://raw.githubusercontent.com/pr0gr4m/Hello-C-World/main/img/%EC%BB%B4%EA%B5%AC/compile.png)  

각 요소들의 간략한 설명은 다음과 같다.  
* 전처리기(preprocessor) : 입력한 데이터를 처리하여 다른 프로그램에 대한 입력으로 사용되는 출력물을 만들어내는 프로그램이다. C 전처리기의 경우 입력받은 소스 파일에 대해 ```#``` 으로 시작하는 전처리기 지시자들을 전처리 문법에 맞게 처리한 소스 파일을 만들어낸다.  
* 컴파일러(compiler) : 특정한 프로그래밍 언어로 작성한 파일을 다른 프로그래밍 언어로 옮기는 번역 프로그램이다. C 컴파일러는 보통 입력 받은 C언어 소스 코드를 번역하여 어셈블리 소스 코드를 생성한다.  
* 어셈블러(assembler) : 어셈블리 소스 코드를 기계어 형태의 오브젝트 코드로 번역하는 프로그램이다. 
    * 오브젝트 파일은 일반적으로 실행 가능한 파일을 만들기 위한 기계어 혹은 기계어에 준하는 이진 코드로 구성된 파일을 말한다. 리눅스에서 사용하는 오브젝트 파일의 종류로는 실행 가능한 파일, 재배치 가능 파일, 공유 오브젝트 파일 세 가지가 있다. 해당 어셈블러 처리 단계를 거친 오브젝트 파일은 재배치 가능 오브젝트 파일이며, 재배치 가능 오브젝트 파일을 모아서 실행 가능한 파일이나 공유 오브젝트 파일을 만들 수 있다.
* 링커(linker) : 하나 이상의 오브젝트 파일을 모아서 단일 실행 파일이나 라이브러리 파일로 병합하는 프로그램이다. 하나 이상의 재배치 가능 오브젝트 파일을 링킹하여 실행 가능한 파일이나 공유 오브젝트 파일을 만든다.  

### 전처리기

### 컴파일러

### 어셈블러

### 링커

## 프로그램의 실행 과정

## 컴파일러와 디버거